{"cells":[{"cell_type":"markdown","metadata":{"id":"lXzrMHFxB6_u"},"source":["/usr/local/lib/python3.10/dist-packages/albumentations/core/bbox_utils.py\n","\n","\n","\n","    bbox=list(bbox)\n","    for i in range(4):\n","      if (bbox[i]<0) :\n","        bbox[i]=0\n","      elif (bbox[i]>1) :\n","        bbox[i]=1\n","    bbox=tuple(bbox)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4795,"status":"ok","timestamp":1683223837822,"user":{"displayName":"Klaudia Szücs","userId":"14356839015226385758"},"user_tz":-120},"id":"NKZOj40zauU3","outputId":"95b67f59-ba5e-4dce-8e62-effe014caff4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4BQ8ajLaxmH"},"outputs":[],"source":["import pandas as pd\n","import albumentations as A\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import os\n","import time\n","import shutil\n","import torch.nn as nn\n","import torchvision\n","import cv2\n","from google.colab.patches import cv2_imshow\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data.sampler import SequentialSampler\n","from torchvision import utils\n","from albumentations import (HorizontalFlip, ShiftScaleRotate, VerticalFlip, Normalize, Flip,\n","                            Compose, GaussNoise)\n","from torchvision.transforms import ToTensor\n","from albumentations.pytorch.transforms import ToTensorV2\n","from torchvision import transforms\n","import glob as glob\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAtHGURDhnvm"},"outputs":[],"source":["# directory where all the images are present\n","# dataset_before\n","# dataset_after_augmented\n","# dataset_after_augmented_2\n","TEST_DIR = \"/content/drive/My Drive/thyroid_nodule_detection/Freehand to Bounding Box Conversion Fix/fixed_labels/cropped_dataset/dataset_after_augmented_2/validation/cropped_images/\"\n","TEST_JSON = \"/content/drive/My Drive/thyroid_nodule_detection/Freehand to Bounding Box Conversion Fix/fixed_labels/cropped_dataset/dataset_after_augmented_2/validation/cropped_labels.json\"\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3715,"status":"ok","timestamp":1683225600283,"user":{"displayName":"Klaudia Szücs","userId":"14356839015226385758"},"user_tz":-120},"id":"Xdu8F0Joa1g3","outputId":"a6790afd-dc49-48df-8b33-4ec4b92120dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n"]}],"source":["classes = [\n","    '__background__', '0', '1' \n","]\n","# classes = [\n","#     '__background__', '2', '3', '4', '5' \n","# ]\n","num_classes = len(classes)\n","print(num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1581,"status":"ok","timestamp":1683225601860,"user":{"displayName":"Klaudia Szücs","userId":"14356839015226385758"},"user_tz":-120},"id":"WLWKyF7oa6HZ","outputId":"d295d39d-a2d3-4e33-e79e-dee7b9849e5f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n","    )\n","  )\n",")"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","# model.to(device)\n","# get the number of input features \n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one with required number of classes\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","model.to(device).eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpDMg2kdmXmO"},"outputs":[],"source":["# Load model\n","checkpoint = torch.load('/content/drive/My Drive/thyroid_nodule_detection/Experiment_5/checkpoints/bestmodel_apr10_with_dataset_after_new_augmented.pt')\n","model.load_state_dict(checkpoint['state_dict'])\n","model.to(device).eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1228,"status":"ok","timestamp":1683225620601,"user":{"displayName":"Klaudia Szücs","userId":"14356839015226385758"},"user_tz":-120},"id":"jxXFSbwbbEiW","outputId":"14cf431f-6b88-4fd7-f0b5-a6383c0cfae4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test instances: 70\n"]}],"source":["test_images = glob.glob(f\"{TEST_DIR}/*.jpg\")\n","print(f\"Test instances: {len(test_images)}\")\n","# to count the total number of images iterated through\n","frame_count = 0\n","# to keep adding the FPS for each image\n","total_fps = 0\n","# this will help to create a different color for each class\n","COLORS = np.random.uniform(0, 255, size=(len(classes), 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uobEB6pGbJPp"},"outputs":[],"source":["detection_threshold = 0.5\n","predicted_bboxes = []\n","predicted_scores = []\n","predicted_bboxes_ =[]\n","\n","for i in range(len(test_images)):\n","    # get the image file name for saving output later on\n","    image_name = test_images[i].split(os.path.sep)[-1].split('.')[0]\n","    image = cv2.imread(test_images[i])\n","    orig_image = image.copy()\n","    # BGR to RGB\n","    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n","    # make the pixel range between 0 and 1\n","    image /= 255.0\n","    # bring color channels to front\n","    image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n","    # convert to tensor\n","    image = torch.tensor(image, dtype=torch.float).cuda()\n","    # add batch dimension\n","    image = torch.unsqueeze(image, 0)\n","    start_time = time.time()\n","    with torch.no_grad():\n","        outputs = model(image.to(device))\n","    end_time = time.time()\n","\n","    # get the current fps\n","    fps = 1 / (end_time - start_time)\n","    # add `fps` to `total_fps`\n","    total_fps += fps\n","    # increment frame count\n","    frame_count += 1\n","    # load all detection to CPU for further operations\n","    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n","    # carry further only if there are detected boxes\n","    if len(outputs[0]['boxes']) != 0:\n","        boxes = outputs[0]['boxes'].data.numpy()\n","        scores = outputs[0]['scores'].data.numpy()\n","        # filter out boxes according to `detection_threshold`\n","        boxes = boxes[scores >= detection_threshold].astype(np.int32)\n","        draw_boxes = boxes.copy()\n","      \n","        # get all the predicited class names\n","        pred_classes = [classes[i] for i in outputs[0]['labels'].cpu().numpy()]\n","        # draw the bounding boxes and write the class name on top of it\n","        for j, box in enumerate(draw_boxes):\n","            class_name = pred_classes[j]\n","           \n","          # making prediction bbox for iou\n","            tensor_bbox = torch.as_tensor([box], dtype=torch.float)\n","            pred_bbox_item = {\n","                  \"case_id\": image_name,\n","                  \"bbox\":tensor_bbox,\n","                  \"labels\":class_name\n","            }\n","            predicted_bboxes.append(pred_bbox_item)\n","\n","          # image printing  \n","            color = COLORS[classes.index(class_name)]\n","            cv2.rectangle(orig_image,\n","                        (int(box[0]), int(box[1])),\n","                        (int(box[2]), int(box[3])),\n","                        color, 2)\n","            cv2.putText(orig_image, class_name, \n","                        (int(box[0]), int(box[1]-5)),\n","                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, \n","                        2, lineType=cv2.LINE_AA)\n","\n","        cv2_imshow(orig_image)\n","        cv2.waitKey(1)\n","        cv2.imwrite(f\"/content/drive/My Drive/thyroid_nodule_detection/Experiment_5/inference_outputs/bestmodel_apr1_with_dataset_after/{image_name}.jpg\", orig_image)\n","\n","    print(f\"Image {i+1} done...\")\n","    print(\"Max score {0}\".format(scores.max()))\n","    predicted_scores.append(scores.max())\n","    print('-'*50)\n","\n","print('TEST PREDICTIONS COMPLETE')\n","cv2.destroyAllWindows()\n","# calculate and print the average FPS\n","avg_fps = total_fps / frame_count\n","print(f\"Average FPS: {avg_fps:.3f}\")\n","avg_score = sum(predicted_scores) / len(predicted_scores)\n","print(f\"Average scores: {avg_score:.3f}\")\n","\n","\n","print(predicted_bboxes)\n"]},{"cell_type":"markdown","metadata":{"id":"lthc7ULSg_Sx"},"source":["# **GET IOU, TEST DATASET, PRECISION AND RECALL**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZPEPlMdRhZq0"},"outputs":[],"source":["class ThyroidNoduleDataset(Dataset):\n","    def __init__(self, data_frame, image_dir,classes,transforms=None, phase='train'):\n","        super().__init__()\n","        self.df = data_frame\n","        self.image_dir = image_dir\n","        self.images = data_frame['case_id'].unique()\n","        self.transforms = transforms\n","        self.classes = classes\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx] + '.jpg'\n","\n","        image_arr = cv2.imread(os.path.join(self.image_dir, image), cv2.IMREAD_COLOR)\n","        image_arr = cv2.cvtColor(image_arr, cv2.COLOR_BGR2RGB).astype(np.float32)\n","        image_arr /= 255.0\n","\n","        image_id = str(image.split('.')[0])\n","        point = self.df[self.df['case_id'] == image_id]\n","        boxes = point[['x', 'y', 'w', 'h']].values\n","        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n","        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n","\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        area = torch.as_tensor(area, dtype=torch.float32)\n","\n","        # there is only one class\n","        labels = []\n","\n","        for box in boxes: \n","          # labels.append(self.classes.index(str(round(self.df.get('tirads').get(idx)[0]))))\n","          labels.append(self.classes.index(str(round(self.df.get('labels').get(idx)))))\n","          # labels.append(self.classes.index(str(round(self.df.get('tirads').get(idx)))))\n","\n","        \n","        labels = torch.as_tensor([lbl for lbl in labels])\n","\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((point.shape[0],), dtype=torch.int64)\n","\n","        target = {}\n","        target['bboxes'] = boxes\n","        target['labels'] = labels\n","\n","        if self.transforms:\n","            sample = {\n","                'image': image_arr,\n","                'bboxes': target['bboxes'],\n","                'labels': target['labels']\n","            }\n","            sample = self.transforms(**sample)\n","            image = sample['image']\n","\n","        target['bboxes'] = torch.stack(tuple(map(torch.tensor,\n","                                                zip(*sample['bboxes'])))).permute(1, 0)\n","        target['boxes'] = target['bboxes'] # later the model needs the 'boxes' naming so keep this copy!\n","\n","        return image, target, image_id"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNhWdnAIFcJT"},"outputs":[],"source":["def get_test_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0),\n","    ], bbox_params={\n","        'format': 'pascal_voc', \n","        'label_fields': ['labels']\n","    })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKWVwDAwFYWW"},"outputs":[],"source":["def process_malignancy(number):\n","  if number <= 3:\n","    return 0\n","  else:\n","    return 1\n","\n","\n","def process_bbox(df):\n","    cases = df\n","    case_list = []\n","    for case in cases:\n","        temp_case = {}\n","        temp_case['case_id'] = case['case_id']\n","        temp_case['tirads'] = case['labels'][0]\n","        temp_case['width'] = 400 # should match cropped images\n","        temp_case['height'] = 350 # should match cropped images\n","        temp_case[\"labels\"] = process_malignancy(temp_case['tirads'])\n","        if(len(case['bboxes']) == 1):\n","          temp_case['x'] = case['bboxes'][0][0]\n","          temp_case['y'] = case['bboxes'][0][1]\n","          temp_case['w'] = case['bboxes'][0][2]\n","          if len(case['bboxes'][0]) < 4:\n","            temp_case['h'] = 1 # bounding box should have height which is greater than 0 if invalid to no break training\n","          else:\n","            temp_case['h'] = case['bboxes'][0][3]\n","          case_list.append(temp_case)\n","        else:\n","          i = len(case['bboxes'])-1\n","          while i >= 0:\n","            temp_case['tirads'] = case['labels'][i]\n","            temp_case[\"labels\"] = process_malignancy(temp_case['tirads'])\n","            temp_case['x'] = case['bboxes'][i][0]\n","            temp_case['y'] = case['bboxes'][i][1]\n","            temp_case['w'] = case['bboxes'][i][2]\n","            if len(case['bboxes'][0]) < 4:\n","              temp_case['h'] = 1 # bounding box should have height which is greater than 0 if invalid to no break training\n","            else:\n","              temp_case['h'] = case['bboxes'][i][3]\n","            case_list.append(temp_case)\n","            i -= 1         \n","    return pd.DataFrame(case_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWathMd8Iv0b"},"outputs":[],"source":["temp_test_df = pd.read_json(TEST_JSON)\n","test_df = process_bbox(temp_test_df['cases'])\n","test_data = ThyroidNoduleDataset(test_df, TEST_DIR,classes,get_test_transform(),  phase='validation')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"631oGVIgg6_T"},"outputs":[],"source":["#predefine ground truth bounding boxes\n","ground_truth_bboxes = []\n","for data in test_data:\n","  tensor_bbox = torch.tensor(data[1]['bboxes'], dtype=torch.float)\n","  ground_truth_bboxes.append({\n","      'case_id': data[2],\n","      'bbox':tensor_bbox,\n","      'label': data[1]['labels'][0]\n","  })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xw86PmxEI5p2"},"outputs":[],"source":["print('PREDICTED BBOXES:', predicted_bboxes)\n","print(len(predicted_bboxes))\n","print('GROUND TRUTH BBOXES:',ground_truth_bboxes)\n","print(len(ground_truth_bboxes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5g54P0ch94h"},"outputs":[],"source":["# Get iou\n","from torchvision import ops\n","\n","ious = []\n","for i,item_i in enumerate(predicted_bboxes):\n","  for j,item_j in enumerate(ground_truth_bboxes):\n","    if item_i['case_id'] == item_j['case_id']:\n","      iou = ops.box_iou(item_j['bbox'],item_i['bbox'])\n","      ious.append(iou)\n","      print(f'{i+1}.TEST CASE IOU : {iou.numpy()[0][0]}', \n","            {item_i['case_id']},\n","            {item_j['case_id']})\n","\n","# all_iou = np.array(thresholds, dtype=np.float32)\n","# print(f'AVARAGE IOU : {np.average(all_iou)}')\n","avg_iou = sum(ious) / len(ground_truth_bboxes)\n","avg_iou = avg_iou.numpy()[0][0]\n","print(f'AVARAGE IOU : {avg_iou * 100}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJKnpoBSfVE_"},"outputs":[],"source":["import numpy\n","import sklearn.metrics\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.metrics import PrecisionRecallDisplay\n","from sklearn.metrics import precision_recall_curve"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QKmofigxpG5K"},"outputs":[],"source":["threshold = detection_threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADWxYpPMzrlp"},"outputs":[],"source":["# print(y_true)\n","# print(y_pred)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAxPeSXtiCiK"},"outputs":[],"source":["y_true =  [1 if test_df['labels'][ind] == 1 else 0 for ind in test_df.index]\n","y_pred = [1 if score >= threshold else 0 for score in predicted_scores]\n","\n","\n","\n","# Confusion Matrix (From Left to Right & Top to Bottom: True Positive, False Negative, False Positive, True Negative)\n","confusion_matrix = numpy.flip(sklearn.metrics.confusion_matrix(y_true, y_pred,labels=[0,1],normalize='all'))\n","cm_display = ConfusionMatrixDisplay(confusion_matrix).plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BluHmtRqk2p3"},"outputs":[],"source":["y_scores = predicted_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnqleWVfeVjM"},"outputs":[],"source":["PrecisionRecallDisplay.from_predictions(y_true, y_scores)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1516,"status":"ok","timestamp":1683223998226,"user":{"displayName":"Klaudia Szücs","userId":"14356839015226385758"},"user_tz":-120},"id":"dBeAln17xrVu","outputId":"f8cc2784-e976-485c-eeb0-57c5f9f6a934"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","    benign 0       0.26      0.30      0.28        20\n"," malignant 1       0.73      0.69      0.70        54\n","\n","    accuracy                           0.58        74\n","   macro avg       0.49      0.49      0.49        74\n","weighted avg       0.60      0.58      0.59        74\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","target_names = ['benign 0', 'malignant 1']\n","print(classification_report(y_true, y_pred, target_names=target_names))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AC5_Gf8llbj"},"outputs":[],"source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import RocCurveDisplay\n","\n","fpr, tpr, _ = roc_curve(y_true, y_scores, pos_label=\"malignant\")\n","roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSzbLtbKPC09"},"outputs":[],"source":["from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, average_precision_score\n","print('Precision: %.3f' % precision_score(y_true, y_pred, pos_label=1))\n","print('Recall: %.3f' % recall_score(y_true, y_pred,pos_label=1))\n","print('Accuracy: %.3f' % accuracy_score(y_true, y_pred))\n","print('F1 Score: %.3f' % f1_score(y_true, y_pred,pos_label=1))\n","print('Average Precesion Score: %.3f' % average_precision_score(y_true, y_pred))\n","print('Average Precesion Score: %.3f' % average_precision_score(y_true, y_pred,pos_label=0))\n","\n","AP_M=average_precision_score(y_true, y_pred,pos_label=1)\n","AP_B=average_precision_score(y_true, y_pred,pos_label=0)\n","\n","ap_scores = [AP_M,AP_B]\n","\n","# Calculate mAP score\n","mAP_score = np.mean(ap_scores)\n","\n","print(\"mAP score:%.3f\" %  mAP_score)\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyNquP31vInclkh36nPIhUkw"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}